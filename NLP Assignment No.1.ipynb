{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77aa9eff",
   "metadata": {},
   "source": [
    "1) What is One-Hot-Encoding\n",
    "\n",
    ":- This one of the type of Text preprocessing2. The aim of text preprocessing2 is to convert words into vectors. When          categorical feature is not datase,then we do one hot encoding. It is one method of converting data to prepare if for        algorithm and get better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98028ff",
   "metadata": {},
   "source": [
    "2) Explain Bag of Words\n",
    "\n",
    ":- It is also known as sentiment analysis, text classification. Representation of text that describes the occurrence of        words within documents. It also includes vocabulary of known words. A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31991552",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Explain Bag of N-Grams\n",
    "\n",
    ":- N-Grams is a sequence. In N-Grams we create multiple combination of given sentence, but that combination should be meaningful.\n",
    "   combinations are:-\n",
    "   combination of one feature = unigram\n",
    "   combination of two feature = bigram\n",
    "   combination of three feature = trigram\n",
    "   combination of four feature = quadgram\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "   combination of n feature = N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a4d6c",
   "metadata": {},
   "source": [
    "4) Explain TF-IDF\n",
    "\n",
    ":- TF-IDF stands for Term Frequency-Inverse Document Frequency and it is a measure, used in the fields of information          retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words,        phrases, lemmas, etc) in a document amongst a collection of documents.\n",
    "\n",
    "   Term Frequency(TF) = No. of repetations of words in a sentence / No.of words in a sentence\n",
    "   \n",
    "   Inverse Document Frequency(IDF) = No. of sentences / No. of sentences containing the word\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b120f",
   "metadata": {},
   "source": [
    "5) What is OOV problem?\n",
    "\n",
    ":- OOV stands for out of vocabulary. Suppose we our corpus contains 4 documents, we apply stopword, vocabulary feature on      our corpus. But later we add new sentence in our corpus, after adding new sentence in corpus the vocabulary & stopword      does not accept this sentence, then we call that sentence is out of vocabulary(OOV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fce05",
   "metadata": {},
   "source": [
    "6) What are word embeddings?\n",
    "\n",
    ":- Word embeddings are a type of word representation that allows words with similar meaning to have a similar                  representation.\n",
    "   For example, words like “mom” and “dad” should be closer together than the words “mom” and “ketchup” or “dad” and          “butter”. Word embeddings are created using a neural network with one input layer, one hidden layer and one output          layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa1740",
   "metadata": {},
   "source": [
    "7) Explain Continuous bag of words (CBOW)\n",
    "\n",
    ":- In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in    the middle. the continuous bag of words model, context is represented by multiple words for a given target words. For      example, we could use “cat” and “tree” as context words for “climbed” as the target word. This calls for a modification    to the neural network architecture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0598ea7",
   "metadata": {},
   "source": [
    "8) Explain SkipGram\n",
    "\n",
    ":- Skip-gram model reverses the use of target and context words. In this case, the target word is fed at the input, the hidden layer remains the same, and the output layer of the neural network is replicated multiple times to accommodate the chosen number of context words. Taking the example of “cat” and “tree” as context words and “climbed” as the target word, the input vector in the skim-gram model would be  [0 0 0 1 0 0 0 0 ]t, while the two output layers would have [0 1 0 0 0 0 0 0] t and [0 0 0 0 0 0 0 1 ]t as target vectors respectively. In place of producing one vector of probabilities, two such vectors would be produced for the current example. The error vector for each output layer is produced in the manner as discussed above. However, the error vectors from all output layers are summed up to adjust the weights via backpropagation. This ensures that weight matrix WO for each output layer remains identical all through training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd79d",
   "metadata": {},
   "source": [
    "9) Explain Glove Embeddings.\n",
    "\n",
    ":- GloVe stands for Global Vectors for word representation. It is an unsupervised learning algorithm. The basic idea behind    the GloVe word embedding is to derive the relationship between the words from statistics.  Unlike the occurrence matrix,    the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the co-occurrence        matrix represents a pair of words occurring together.\n",
    "   This is the idea behind the GloVe pre-trained word embeddings, and it is expressed as;\n",
    "   F(Wi,Wj,Wk) = Pik / Pjk "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
