{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab7cc4b",
   "metadata": {},
   "source": [
    "1) Explain the basic architecture of RNN cell.\n",
    "\n",
    ":- RNN is recurrent neural network. It used to usecases like Word2Vec, Embedding layers, doc2vec, transformers, encoder-      decoder, bidirectional LSTM. RNN cell is also called as backborn of the RNN networks.On a mathematical level, a sequence    of inputs are passed through an RNN cell, one at a time. The state of the cell helps it remember the past sequence and      combine that information with the current input to provide an output. The most commonly used RNN cell are GRU & LSTM,      both this cells have gate present in them, which are essentially value between 0 & 1 correspond to each input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161dac3",
   "metadata": {},
   "source": [
    "2) Explain Backpropagation through time (BPTT)\n",
    "\n",
    ":-  The application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a       time series. A recurrent neural network shows one input each timestep & predicts one output. In bcl propogation some       components are depend on weights & bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d760f2f",
   "metadata": {},
   "source": [
    "3) Explain Vanishing and exploding gradients\n",
    "\n",
    ":- The parameters of the higher layers change significantly whereas the parameters of lower layers would not change much      (or not at all).The model weight become 0 during training. The model learns very slowly and perhaps the training            stagnates at a very early stage just after a few iterations. \n",
    "   In exploding gradients, There is an exponential growth in the model parameters. The model weights may become NaN during    training & The model experiences  avalanche learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94492df4",
   "metadata": {},
   "source": [
    "4) Explain Long short-term memory (LSTM)\n",
    "\n",
    ":- LSTM stands for Long-Short Term Memory. LSTM is a type of recurrent neural network but is better than traditional          recurrent neural networks in terms of memory. Having a good hold over memorizing certain patterns LSTMs perform fairly      better. As with every other NN, LSTM can have multiple hidden layers and as it passes through every layer, the relevant    information is kept and all the irrelevant information gets discarded in every single cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5eb754",
   "metadata": {},
   "source": [
    "5) Explain Gated recurrent unit (GRU)\n",
    "\n",
    ":- GRU is one of the most commonly used RNN cell. GRU are gating mechanism neural network, GRU's performance on certain        tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to      that of LSTM. There are several variations on the full gated unit, with gating done using the previous hidden state and    the bias in various combinations, and a simplified form called minimal gated unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19a22e",
   "metadata": {},
   "source": [
    "6) Explain Peephole LSTM\n",
    "\n",
    ":- This is also one of the type of LSTM. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding          “peephole connections.” This means that we let the gate layers look at the cell state. In this peephole connection we      can see that all the gates are having an input along with the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1d096",
   "metadata": {},
   "source": [
    "7) Bidirectional RNNs\n",
    "\n",
    ":- A bidirectional RNN is the combination of RNNs training the network in opposite directions, one from the beginning to     the end of a sequence & other from end of sequence to the begining. It helps in analyzing the future events by not         limiting the model's learning to past and present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03eb9b4",
   "metadata": {},
   "source": [
    "9) Explain BiLSTM\n",
    "\n",
    ":- Bidirectional long-short term memory(bi-lstm) is the process of making any neural network o have the sequence              information in both directions backwards (future to past) or forward(past to future). In bidirectional, our input flows    in two directions, making a bi-lstm different from the regular LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49438698",
   "metadata": {},
   "source": [
    "10) Explain BiGRU\n",
    "\n",
    ":- A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a          forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the    input and forget gates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
